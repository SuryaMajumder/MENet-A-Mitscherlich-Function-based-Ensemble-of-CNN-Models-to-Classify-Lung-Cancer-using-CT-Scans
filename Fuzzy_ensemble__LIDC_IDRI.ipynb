{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KffHnnQycKEt"
      },
      "source": [
        "# **Operations to be done with google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSJFL0V0aMok",
        "outputId": "1f116d54-be79-4a63-f259-0cf87c0c16ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Linking with Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXKsyAjnY7sK"
      },
      "source": [
        "# **Data Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iGUKu6hx10-O"
      },
      "outputs": [],
      "source": [
        "#@title Requirements\n",
        "\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choosing Resources\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "   tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IbihwctvY6lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Cleansing\n",
        "\n",
        "import cv2\n",
        "import imghdr\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data_dir = ['/content/drive/My Drive/Colab Notebooks/data']\n",
        "\n",
        "image_exts = ['jpg', 'jpeg', 'png', 'bmp']\n",
        "\n",
        "for i in data_dir:\n",
        "    for image_class in os.listdir(i):\n",
        "        for image in os.listdir(os.path.join(i, image_class)):\n",
        "            image_path = os.path.join(i, image_class, image)\n",
        "            try:\n",
        "                img = cv2.imread(image_path)\n",
        "                tip = imghdr.what(image_path)\n",
        "                if tip not in image_exts:\n",
        "                    print('Image not in ext list {}'.format(image_path))\n",
        "                    os.remove(image_path)\n",
        "            except Exception as e:\n",
        "                print('Issue with image {}'.format(image_path))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "I6YHjYVQZBSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "huZqVE0-T3h5"
      },
      "outputs": [],
      "source": [
        "#@title Creating Data Pipeline\n",
        "import numpy as np\n",
        "\n",
        "data1 = tf.keras.utils.image_dataset_from_directory('/content/drive/My Drive/Colab Notebooks/data/train', shuffle=True)\n",
        "data2 = tf.keras.utils.image_dataset_from_directory('/content/drive/My Drive/Colab Notebooks/data/test', shuffle=True)\n",
        "data3 = tf.keras.utils.image_dataset_from_directory('/content/drive/My Drive/Colab Notebooks/data/val', shuffle=True)\n",
        "\n",
        "data_iterator1 = data1.as_numpy_iterator()\n",
        "data_iterator2 = data2.as_numpy_iterator()\n",
        "data_iterator3 = data3.as_numpy_iterator()\n",
        "batch1 = data_iterator1.next()\n",
        "print(\"Batch 1 classes: \", batch1[1])\n",
        "batch2 = data_iterator2.next()\n",
        "print(\"Batch 2 classes: \", batch2[1])\n",
        "batch3 = data_iterator3.next()\n",
        "print(\"Batch 3 classes: \", batch3[1])\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, img in enumerate(batch1[0][:4]):\n",
        "    ax[idx].imshow(img.astype(int))\n",
        "    ax[idx].title.set_text(batch1[1][idx])\n",
        "\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, img in enumerate(batch2[0][:4]):\n",
        "    ax[idx].imshow(img.astype(int))\n",
        "    ax[idx].title.set_text(batch2[1][idx])\n",
        "\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx, img in enumerate(batch3[0][:4]):\n",
        "    ax[idx].imshow(img.astype(int))\n",
        "    ax[idx].title.set_text(batch3[1][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT0LWAfOT5qa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Preprocessing Data\n",
        "\n",
        "''' Scaling Data '''\n",
        "# Scaling Data\n",
        "data1 = data1.map(lambda x,y: (x/255, y))\n",
        "data1.as_numpy_iterator().next()[0].max()\n",
        "\n",
        "data2 = data2.map(lambda x,y: (x/255, y))\n",
        "data2.as_numpy_iterator().next()[0].max()\n",
        "\n",
        "data3 = data3.map(lambda x,y: (x/255, y))\n",
        "data3.as_numpy_iterator().next()[0].max()\n",
        "\n",
        "\n",
        "''' Splitting Data '''\n",
        "train_size = int(len(data1))\n",
        "val_size = int(len(data3))\n",
        "test_size = int(len(data2))\n",
        "\n",
        "train = data1.take(train_size)\n",
        "val = data3.take(val_size)\n",
        "test = data2.take(test_size)\n",
        "\n",
        "print(train_size, \"+\", test_size, \"+\", val_size, \"=\", len(data1)+len(data2)+len(data3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Assigning weights to classes\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "labels = []\n",
        "for batch in train.as_numpy_iterator():\n",
        "    X1, y1 = batch\n",
        "    labels.extend(list(y1))\n",
        "\n",
        "labels = np.array(labels)\n",
        "print(labels)\n",
        "\n",
        "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(labels), y = labels)\n",
        "class_weights = dict(zip(np.unique(labels), class_weights))"
      ],
      "metadata": {
        "id": "TzhoFGvZfWcg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNjz3dlbZPEL"
      },
      "source": [
        "# **Deep Learning Models Creation, Training, Evaluating, Saving, Performance Graph etc.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAIUbksF3w3X",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Deep Models Xception, InceptionResnetV2, MobileNetV2\n",
        "\n",
        "''' Deep Model '''\n",
        "# The Network\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Dropout, AveragePooling2D, Concatenate, GlobalAveragePooling2D, BatchNormalization, ReLU, Add, SeparableConv2D\n",
        "from tensorflow.keras.applications import MobileNetV2, InceptionResNetV2, Xception\n",
        "\n",
        "\n",
        "def xception(img_shape, n_classes):\n",
        "    xceptionnet = Xception(input_shape=img_shape, include_top=False, weights='imagenet')\n",
        "    xceptionnet.trainable = True\n",
        "\n",
        "    input = Input(img_shape)\n",
        "    x = xceptionnet(input, training=True)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def inceptionresnetv2(img_shape, n_classes):\n",
        "    inceptionresnet = InceptionResNetV2(input_shape=img_shape, include_top=False, weights='imagenet')\n",
        "    inceptionresnet.trainable = True\n",
        "\n",
        "    input = Input(img_shape)\n",
        "    x = inceptionresnet(input, training=True)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mobilenetv2(img_shape, n_classes):\n",
        "    mobilenet = MobileNetV2(input_shape=img_shape, include_top=False, weights='imagenet')\n",
        "    mobilenet.trainable = True\n",
        "\n",
        "    input = Input(img_shape)\n",
        "    x = mobilenet(input, training=True)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_shape = (256, 256, 3)\n",
        "n_classes = 3\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "opt = SGD(learning_rate=0.1)\n",
        "\n",
        "model1 = xception(input_shape, n_classes)\n",
        "model1.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model1.summary()\n",
        "\n",
        "model2 = inceptionresnetv2(input_shape, n_classes)\n",
        "model2.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()\n",
        "\n",
        "model3 = mobilenetv2(input_shape, n_classes)\n",
        "model3.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the Models\n",
        "\n",
        "# Train\n",
        "logdir='/content/drive/My Drive/Colab Notebooks/logs'\n",
        "tensorboard_callback11 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "tensorboard_callback12 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
        "hist1 = model1.fit(train, class_weight=class_weights epochs=60, validation_data=val, callbacks=[tensorboard_callback11, tensorboard_callback12])\n",
        "print(hist1.history)\n",
        "\n",
        "tensorboard_callback21 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "tensorboard_callback22 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
        "hist2 = model2.fit(train, class_weight=class_weights epochs=60, validation_data=val, callbacks=[tensorboard_callback21, tensorboard_callback22])\n",
        "print(hist2.history)\n",
        "\n",
        "tensorboard_callback31 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "tensorboard_callback32 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
        "hist3 = model3.fit(train, class_weight=class_weights epochs=60, validation_data=val, callbacks=[tensorboard_callback31, tensorboard_callback32])\n",
        "print(hist3.history)"
      ],
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.status.busy": "2023-09-04T04:41:37.485166Z",
          "iopub.execute_input": "2023-09-04T04:41:37.485523Z",
          "iopub.status.idle": "2023-09-04T05:21:41.665786Z",
          "shell.execute_reply.started": "2023-09-04T04:41:37.485491Z",
          "shell.execute_reply": "2023-09-04T05:21:41.664780Z"
        },
        "trusted": true,
        "id": "j-ReXPx-uTk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating the Models\n",
        "\n",
        "''' Evaluation '''\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, sparse_categorical_accuracy\n",
        "from sklearn.metrics import *\n",
        "\n",
        "loss1, results1 = model1.evaluate(test, batch_size=328, callbacks=[tensorboard_callback11, tensorboard_callback12])\n",
        "loss2, results2 = model2.evaluate(test, batch_size=328, callbacks=[tensorboard_callback21, tensorboard_callback22])\n",
        "loss3, results3 = model3.evaluate(test, batch_size=328, callbacks=[tensorboard_callback31, tensorboard_callback32])\n",
        "print(\"Result1 : \\n\", results1, \"\\nResult2 : \\n\", results2, \"\\nResult3 : \\n\", results3)\n",
        "print(\"Loss1: \\n\", loss1, \"\\nLoss2: \\n\", loss2, \"\\nLoss3: \\n\", loss3)\n",
        "\n",
        "''' Xception '''\n",
        "print(\"For Xception: \")\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "labels = []\n",
        "predicts = []\n",
        "pre1 = Precision()\n",
        "re1 = Recall()\n",
        "acc1 = BinaryAccuracy()\n",
        "for batch1 in test.as_numpy_iterator():\n",
        "    X1, y1 = batch1\n",
        "    labels.extend(list(y1))\n",
        "    yhat1 = model1.predict(X1, verbose=0)\n",
        "    max = a = 0\n",
        "    arr = np.ones(yhat1.shape[0])*-1\n",
        "    for i in yhat1:\n",
        "        max = np.argmax(i)\n",
        "        arr[a] = max\n",
        "        a+=1\n",
        "    predicts.extend(list(arr))\n",
        "    pre1.update_state(y1, arr)\n",
        "    re1.update_state(y1, arr)\n",
        "    acc1.update_state(y1, arr)\n",
        "acc11 = sparse_categorical_accuracy(y1, yhat1)\n",
        "assert acc11.shape == (y1.shape[0],)\n",
        "print(f'Precision:{pre1.result().numpy()}, Recall:{re1.result().numpy()}, Accuracy (Binary):{acc1.result().numpy()}, Accuracy (sparse_categorical):{acc11.numpy()}')\n",
        "matrix = confusion_matrix(y1, arr)\n",
        "print(\"Confusion Matrix for Model 1: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"For Test Dataset: \")\n",
        "pre.update_state(labels, predicts)\n",
        "re.update_state(labels, predicts)\n",
        "acc.update_state(labels, predicts)\n",
        "print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy (Binary):{acc.result().numpy()}')\n",
        "matrix = confusion_matrix(labels, predicts)\n",
        "print(\"Confusion Matrix for Xception on Test Dataset: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "''' InceptionResNetV2 '''\n",
        "print(\"For InceptionResNetV2: \")\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "labels = []\n",
        "predicts = []\n",
        "pre2 = Precision()\n",
        "re2 = Recall()\n",
        "acc2 = BinaryAccuracy()\n",
        "for batch2 in test.as_numpy_iterator():\n",
        "    X2, y2 = batch2\n",
        "    labels.extend(list(y2))\n",
        "    yhat2 = model2.predict(X2)\n",
        "    max = a = 0\n",
        "    arr = np.ones(yhat2.shape[0])*-1\n",
        "    for i in yhat2:\n",
        "        max = np.argmax(i)\n",
        "        arr[a] = max\n",
        "        a+=1\n",
        "    predicts.extend(list(arr))\n",
        "    pre2.update_state(y2, arr)\n",
        "    re2.update_state(y2, arr)\n",
        "    acc2.update_state(y2, arr)\n",
        "acc21 = sparse_categorical_accuracy(y2, yhat2)\n",
        "assert acc21.shape == (y2.shape[0],)\n",
        "print(f'Precision:{pre2.result().numpy()}, Recall:{re2.result().numpy()}, Accuracy (Binary):{acc2.result().numpy()}, Accuracy (sparse_categorical):{acc21.numpy()}')\n",
        "matrix = confusion_matrix(y2, arr)\n",
        "print(\"Confusion Matrix for Model 2: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"For Test Dataset: \")\n",
        "pre.update_state(labels, predicts)\n",
        "re.update_state(labels, predicts)\n",
        "acc.update_state(labels, predicts)\n",
        "print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy (Binary):{acc.result().numpy()}')\n",
        "matrix = confusion_matrix(labels, predicts)\n",
        "print(\"Confusion Matrix for InceptionResNetV2 on Test Dataset: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "''' MobileNetV2 '''\n",
        "print(\"For MobileNetV2: \")\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "labels = []\n",
        "predicts = []\n",
        "pre3 = Precision()\n",
        "re3 = Recall()\n",
        "acc3 = BinaryAccuracy()\n",
        "for batch3 in test.as_numpy_iterator():\n",
        "    X3, y3 = batch3\n",
        "    labels.extend(list(y3))\n",
        "    yhat3 = model3.predict(X3)\n",
        "    max = a = 0\n",
        "    arr = np.ones(yhat3.shape[0])*-1\n",
        "    for i in yhat3:\n",
        "        max = np.argmax(i)\n",
        "        arr[a] = max\n",
        "        a+=1\n",
        "    predicts.extend(list(arr))\n",
        "    pre3.update_state(y3, arr)\n",
        "    re3.update_state(y3, arr)\n",
        "    acc3.update_state(y3, arr)\n",
        "acc31 = sparse_categorical_accuracy(y3, yhat3)\n",
        "assert acc31.shape == (y3.shape[0],)\n",
        "print(f'Precision:{pre3.result().numpy()}, Recall:{re3.result().numpy()}, Accuracy (Binary):{acc3.result().numpy()}, Accuracy (sparse_categorical):{acc31.numpy()}')\n",
        "matrix = confusion_matrix(y3, arr)\n",
        "print(\"Confusion Matrix for Model 3: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "display.plot()\n",
        "plt.show()\n",
        "\n",
        "print(\"For Test Dataset: \")\n",
        "pre.update_state(labels, predicts)\n",
        "re.update_state(labels, predicts)\n",
        "acc.update_state(labels, predicts)\n",
        "print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy (Binary):{acc.result().numpy()}')\n",
        "matrix = confusion_matrix(labels, predicts)\n",
        "print(\"Confusion Matrix for MobileNetV2 on Test Dataset: \\n\", matrix)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-04T05:23:54.087076Z",
          "iopub.execute_input": "2023-09-04T05:23:54.087453Z",
          "iopub.status.idle": "2023-09-04T05:24:56.247555Z",
          "shell.execute_reply.started": "2023-09-04T05:23:54.087421Z",
          "shell.execute_reply": "2023-09-04T05:24:56.246502Z"
        },
        "trusted": true,
        "cellView": "form",
        "id": "VGMqZ1FNuhqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hBbI8OQlAPBK"
      },
      "outputs": [],
      "source": [
        "#@title Saving the Models and their Histories\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Saving the Models\n",
        "\n",
        "model1.save(os.path.join('/content/drive/My Drive/Colab Notebooks/models','imageclassifier1.h5'))\n",
        "model2.save(os.path.join('/content/drive/My Drive/Colab Notebooks/models','imageclassifier2.h5'))\n",
        "model3.save(os.path.join('/content/drive/My Drive/Colab Notebooks/models','imageclassifier3.h5'))\n",
        "\n",
        "\n",
        "# Saving the Model Histories\n",
        "\n",
        "with open(os.path.join(logdir+'/history1.pkl'), 'wb') as file:\n",
        "    pickle.dump(hist1, file)\n",
        "\n",
        "with open(os.path.join(logdir+'/history2.pkl'), 'wb') as file:\n",
        "    pickle.dump(hist2, file)\n",
        "\n",
        "with open(os.path.join(logdir+'/history3.pkl'), 'wb') as file:\n",
        "    pickle.dump(hist3, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_aEnAxJpjcsP"
      },
      "outputs": [],
      "source": [
        "#@title Re-Loading the Models and their Histories\n",
        "\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "logdir='/content/drive/My Drive/Colab Notebooks/logs'\n",
        "\n",
        "# Loading the Models\n",
        "model1 = load_model('/content/drive/My Drive/Colab Notebooks/models/imageclassifier1.h5')\n",
        "model2 = load_model('/content/drive/My Drive/Colab Notebooks/models/imageclassifier2.h5')\n",
        "model3 = load_model('/content/drive/My Drive/Colab Notebooks/models/imageclassifier3.h5')\n",
        "\n",
        "\n",
        "# Loading the Model Histories\n",
        "\n",
        "with open(os.path.join(logdir+'/history1.pkl'), 'rb') as file:\n",
        "    hist1 = pickle.load(file)\n",
        "\n",
        "with open(os.path.join(logdir+'/history2.pkl'), 'rb') as file:\n",
        "    hist2 = pickle.load(file)\n",
        "\n",
        "with open(os.path.join(logdir+'/history3.pkl'), 'rb') as file:\n",
        "    hist3 = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Performance Graphs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "''' Model 1 '''\n",
        "# Loss\n",
        "fig = plt.figure()\n",
        "plt.title(\"Xception\\n\\n\", loc='center', fontsize=26)\n",
        "plt.plot(hist1.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist1.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist1.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist1.history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "''' Model 2 '''\n",
        "# Loss\n",
        "fig = plt.figure()\n",
        "plt.title(\"InceptionResNetV2\\n\\n\", loc='center', fontsize=26)\n",
        "plt.plot(hist2.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist2.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist2.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist2.history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "''' Model 3 '''\n",
        "# Loss\n",
        "fig = plt.figure()\n",
        "plt.title(\"MobileNetV2\\n\\n\", loc='center', fontsize=26)\n",
        "plt.plot(hist3.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist3.history['val_loss'], color='orange', label='val_loss')\n",
        "fig.suptitle('Loss', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist3.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist3.history['val_accuracy'], color='orange', label='val_accuracy')\n",
        "fig.suptitle('Accuracy', fontsize=20)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.status.busy": "2023-09-04T05:25:06.386723Z",
          "iopub.execute_input": "2023-09-04T05:25:06.387091Z",
          "iopub.status.idle": "2023-09-04T05:25:07.945401Z",
          "shell.execute_reply.started": "2023-09-04T05:25:06.387061Z",
          "shell.execute_reply": "2023-09-04T05:25:07.944483Z"
        },
        "trusted": true,
        "id": "NwyddxH0wTc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3rXsrzvxHCi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ROC-AUC Curves\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import math, os\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "#ROC-AUC\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_roc(val_label,decision_val, caption='ROC Curve'):\n",
        "    num_classes=np.unique(val_label).shape[0]\n",
        "    classes = []\n",
        "    for i in range(num_classes):\n",
        "        classes.append(i)\n",
        "    plt.figure()\n",
        "    decision_val = label_binarize(decision_val, classes=classes)\n",
        "\n",
        "    if num_classes!=2:\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(num_classes):\n",
        "            y_val = label_binarize(val_label, classes=classes)\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_val[:, i], decision_val[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        for i in range(num_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i+1, roc_auc[i]))\n",
        "    else:\n",
        "        fpr,tpr,_ = roc_curve(val_label,decision_val, pos_label=1)\n",
        "        roc_auc = auc(fpr,tpr)*100\n",
        "        plt.plot(fpr,tpr,label='ROC curve (AUC=%0.2f)'%roc_auc)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(caption)\n",
        "    plt.legend(loc=\"lower right\",  bbox_to_anchor=(1, 0.5))\n",
        "    plt.savefig(str(len(classes))+'.png',dpi=300)\n",
        "\n",
        "def predicting(ensemble_prob):\n",
        "    prediction = np.zeros((ensemble_prob.shape[0],))\n",
        "    for i in range(ensemble_prob.shape[0]):\n",
        "        temp = ensemble_prob[i]\n",
        "        t = np.where(temp == np.max(temp))[0][0]\n",
        "        prediction[i] = t\n",
        "    return prediction\n",
        "\n",
        "def metrics(labels, predictions, classes, display_label):\n",
        "    print(\"Classification Report:\")\n",
        "    print(\"Labels : \",labels.shape, '\\nPredictions : ', predictions.shape)\n",
        "    print(classification_report(labels, predictions, target_names = classes, digits = 4))\n",
        "    matrix = confusion_matrix(labels, predictions)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(matrix)\n",
        "    plt.figure()\n",
        "    display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = display_label)\n",
        "    display.plot()\n",
        "    plt.xticks(rotation=50)\n",
        "    plt.show()\n",
        "    print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal()/matrix.sum(axis = 1)))\n",
        "    print(\"\\nBalanced Accuracy Score: \",balanced_accuracy_score(labels,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Precision, Recall and Accuracy of the Models with respect to test dataset\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "labels = []\n",
        "p1 = []; p2 = []; p3 = []\n",
        "for batch1 in test.as_numpy_iterator():\n",
        "    X1, y1 = batch1\n",
        "    labels.extend(list(y1))\n",
        "    yhat1 = model1.predict(X1, verbose=0)\n",
        "    yhat2 = model2.predict(X1, verbose=0)\n",
        "    yhat3 = model3.predict(X1, verbose=0)\n",
        "    max = 0\n",
        "    for i in yhat1:\n",
        "        max = np.argmax(i)\n",
        "        p1.append(max)\n",
        "\n",
        "    max = 0\n",
        "    for i in yhat2:\n",
        "        max = np.argmax(i)\n",
        "        p2.append(max)\n",
        "\n",
        "    max = 0\n",
        "    for i in yhat3:\n",
        "        max = np.argmax(i)\n",
        "        p3.append(max)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "p1 = np.asarray(p1)\n",
        "p2 = np.asarray(p2)\n",
        "p3 = np.asarray(p3)\n",
        "\n",
        "print(\"Xception : \", end=\"\")\n",
        "for i in p1:\n",
        "    print(i, end=\" \")\n",
        "print(\"\\nInceptionResNetV2 : \", end=\"\")\n",
        "for i in p2:\n",
        "    print(i, end = \" \")\n",
        "print(\"\\nMobileNetV2 : \", end=\"\")\n",
        "for i in p3:\n",
        "    print(i, end=\" \")\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"For Xception : \")\n",
        "metrics(labels, p1, ['1', '2'], ['Benign', 'Malignant'])\n",
        "plot_roc(labels, p1)\n",
        "\n",
        "avg_acc_list = []\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "acc_fold = accuracy_score(labels, p1)\n",
        "avg_acc_list.append(acc_fold)\n",
        "precision_fold = precision_score(labels, p1, average='macro')\n",
        "avg_precision_list.append(precision_fold)\n",
        "recall_fold = recall_score(labels, p1, average='macro')\n",
        "avg_recall_list.append(recall_fold)\n",
        "f1_fold  = f1_score(labels, p1, average='macro')\n",
        "avg_f1_list.append(f1_fold)\n",
        "\n",
        "print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
        "print('________________________________________________________________')\n",
        "\n",
        "avg_acc = np.asarray(avg_acc_list)\n",
        "avg_pre = np.asarray(avg_precision_list)\n",
        "avg_recall = np.asarray(avg_recall_list)\n",
        "avg_f1 = np.asarray(avg_f1_list)\n",
        "print(\"\\n\")\n",
        "print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))\n",
        "\n",
        "\n",
        "print(\"For InceptionResNetV2 : \")\n",
        "metrics(labels, p2, ['1', '2'], ['Benign', 'Malignant'])\n",
        "plot_roc(labels, p2)\n",
        "\n",
        "avg_acc_list = []\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "acc_fold = accuracy_score(labels, p2)\n",
        "avg_acc_list.append(acc_fold)\n",
        "precision_fold = precision_score(labels, p2, average='macro')\n",
        "avg_precision_list.append(precision_fold)\n",
        "recall_fold = recall_score(labels, p2, average='macro')\n",
        "avg_recall_list.append(recall_fold)\n",
        "f1_fold  = f1_score(labels, p2, average='macro')\n",
        "avg_f1_list.append(f1_fold)\n",
        "\n",
        "print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
        "print('________________________________________________________________')\n",
        "\n",
        "avg_acc = np.asarray(avg_acc_list)\n",
        "avg_pre = np.asarray(avg_precision_list)\n",
        "avg_recall = np.asarray(avg_recall_list)\n",
        "avg_f1 = np.asarray(avg_f1_list)\n",
        "print(\"\\n\")\n",
        "print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))\n",
        "\n",
        "\n",
        "print(\"For MobileNetV2 : \")\n",
        "metrics(labels, p3, ['1', '2'], ['Benign', 'Malignant'])\n",
        "plot_roc(labels, p3)\n",
        "\n",
        "avg_acc_list = []\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "acc_fold = accuracy_score(labels, p3)\n",
        "avg_acc_list.append(acc_fold)\n",
        "precision_fold = precision_score(labels, p3, average='macro')\n",
        "avg_precision_list.append(precision_fold)\n",
        "recall_fold = recall_score(labels, p3, average='macro')\n",
        "avg_recall_list.append(recall_fold)\n",
        "f1_fold  = f1_score(labels, p3, average='macro')\n",
        "avg_f1_list.append(f1_fold)\n",
        "\n",
        "print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
        "print('________________________________________________________________')\n",
        "\n",
        "avg_acc = np.asarray(avg_acc_list)\n",
        "avg_pre = np.asarray(avg_precision_list)\n",
        "avg_recall = np.asarray(avg_recall_list)\n",
        "avg_f1 = np.asarray(avg_f1_list)\n",
        "print(\"\\n\")\n",
        "print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))"
      ],
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.status.busy": "2023-09-04T05:26:08.537582Z",
          "iopub.execute_input": "2023-09-04T05:26:08.537989Z",
          "iopub.status.idle": "2023-09-04T05:26:37.512876Z",
          "shell.execute_reply.started": "2023-09-04T05:26:08.537949Z",
          "shell.execute_reply": "2023-09-04T05:26:37.511955Z"
        },
        "trusted": true,
        "id": "yBBhjMbEwdC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgRByXkyZ3Ku"
      },
      "source": [
        "# **Predictions by each model and Probability Extraction for Fuzzy Ensemble of Deep Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkxFb0uuNHZ6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Predictions of each Model and Probability Extraction from Models\n",
        "\n",
        "# Getting Probability Distribution\n",
        "\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "p1 = []\n",
        "p2 = []\n",
        "p3 = []\n",
        "labels = []\n",
        "for batch1 in test.as_numpy_iterator():\n",
        "    X1, y1 = batch1\n",
        "    labels.extend(list(y1))\n",
        "    yhat1 = model1.predict(X1, verbose=0)\n",
        "    yhat2 = model2.predict(X1, verbose=0)\n",
        "    yhat3 = model3.predict(X1, verbose=0)\n",
        "\n",
        "    for i in yhat1:\n",
        "        p1.append(i)\n",
        "    for i in yhat2:\n",
        "        p2.append(i)\n",
        "    for i in yhat3:\n",
        "        p3.append(i)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "p1 = np.array(p1)\n",
        "p2 = np.array(p2)\n",
        "p3 = np.array(p3)\n",
        "\n",
        "import csv\n",
        "fp = open(\"/content/drive/My Drive/Colab Notebooks/csv/xception.csv\",'w+',newline = '')\n",
        "writer = csv.writer(fp)\n",
        "for i in p1:\n",
        "    writer.writerow([i[0], i[1], i[2]])\n",
        "fp.close()\n",
        "\n",
        "fp = open(\"/content/drive/My Drive/Colab Notebooks/csv/inceptionresnetv2.csv\",'w+',newline = '')\n",
        "writer = csv.writer(fp)\n",
        "for i in p2:\n",
        "    writer.writerow([i[0], i[1], i[2]])\n",
        "fp.close()\n",
        "\n",
        "fp = open(\"/content/drive/My Drive/Colab Notebooks/csv/mobilenetv2.csv\",'w+',newline = '')\n",
        "writer = csv.writer(fp)\n",
        "for i in p3:\n",
        "    writer.writerow([i[0], i[1], i[2]])\n",
        "fp.close()\n",
        "\n",
        "fp = open(\"/content/drive/My Drive/Colab Notebooks/csv/labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(fp)\n",
        "for i in labels:\n",
        "    writer.writerow([i])\n",
        "fp.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eFXAngThuGbU"
      },
      "outputs": [],
      "source": [
        "#@title ROC-AUC Curves\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import math, os\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "#ROC-AUC\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_roc(val_label,decision_val, caption='ROC Curve'):\n",
        "    num_classes=np.unique(val_label).shape[0]\n",
        "    classes = []\n",
        "    for i in range(num_classes):\n",
        "        classes.append(i)\n",
        "    plt.figure()\n",
        "    decision_val = label_binarize(decision_val, classes=classes)\n",
        "\n",
        "    if num_classes!=2:\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(num_classes):\n",
        "            y_val = label_binarize(val_label, classes=classes)\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_val[:, i], decision_val[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        for i in range(num_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i+1, roc_auc[i]))\n",
        "    else:\n",
        "        fpr,tpr,_ = roc_curve(val_label,decision_val, pos_label=1)\n",
        "        roc_auc = auc(fpr,tpr)*100\n",
        "        plt.plot(fpr,tpr,label='ROC curve (AUC=%0.2f)'%roc_auc)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(caption)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(str(len(classes))+'.png',dpi=300)\n",
        "\n",
        "def predicting(ensemble_prob):\n",
        "    prediction = np.zeros((ensemble_prob.shape[0],))\n",
        "    for i in range(ensemble_prob.shape[0]):\n",
        "        temp = ensemble_prob[i]\n",
        "        t = np.where(temp == np.max(temp))[0][0]\n",
        "        prediction[i] = t\n",
        "    return prediction\n",
        "\n",
        "def metrics(labels, predictions, classes):\n",
        "    print(\"Classification Report:\")\n",
        "    print(\"Labels : \",labels.shape, '\\nPredictions : ', predictions.shape)\n",
        "    print(classification_report(labels, predictions, target_names = classes, digits = 4))\n",
        "    matrix = confusion_matrix(labels, predictions)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(matrix)\n",
        "    display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = ['Benign', 'Malignant'])\n",
        "    display.plot()\n",
        "    plt.show()\n",
        "    print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal()/matrix.sum(axis = 1)))\n",
        "    print(\"\\nBalanced Accuracy Score: \",balanced_accuracy_score(labels,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Fuzzy Ensemble using Mitscherlich Function***"
      ],
      "metadata": {
        "id": "elLENXLqrgrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Predictions with Mitscherlich Function\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def fuzzy_rank(CF, top):\n",
        "    R_L = np.zeros(CF.shape)\n",
        "    for i in range(CF.shape[0]):\n",
        "        for j in range(CF.shape[1]):\n",
        "            for k in range(CF.shape[2]):\n",
        "                R_L[i][j][k] = (2 - 1 * 2 ** CF[i][j][k])\n",
        "\n",
        "    K_L = 1*np.ones(shape = R_L.shape)\n",
        "    for i in range(R_L.shape[0]):\n",
        "        for sample in range(R_L.shape[1]):\n",
        "            for k in range(top-1):\n",
        "                a = R_L[i][sample]\n",
        "                idx = np.where(a==np.partition(a, k)[k])\n",
        "                K_L[i][sample][idx] = R_L[i][sample][idx]\n",
        "\n",
        "    return K_L\n",
        "\n",
        "def CFS_func(CF, K_L):\n",
        "    H = CF.shape[0]\n",
        "    for f in range(CF.shape[0]):\n",
        "        for i in range(CF.shape[1]):\n",
        "            idx = np.where(K_L[f][i] == 1)\n",
        "            CF[f][i][idx] = 0\n",
        "    CFS = 1 - np.sum(CF,axis=0)/H\n",
        "    return CFS\n",
        "\n",
        "def Mitcherlich(top = 2, *argv):\n",
        "    L = 0\n",
        "    for arg in argv:\n",
        "        L += 1\n",
        "\n",
        "    num_classes = arg.shape[1]\n",
        "    CF = np.zeros(shape = (L,arg.shape[0], arg.shape[1]))\n",
        "\n",
        "    for i, arg in enumerate(argv):\n",
        "        CF[:][:][i] = arg\n",
        "\n",
        "    R_L = fuzzy_rank(CF, top)\n",
        "\n",
        "    RS = np.sum(R_L, axis=0)\n",
        "    CFS = CFS_func(CF, R_L)\n",
        "    FS = RS*CFS\n",
        "\n",
        "    import csv\n",
        "    fp = open(\"/content/drive/My Drive/Colab Notebooks/csv/ensemble.csv\",'w+',newline = '')\n",
        "    writer = csv.writer(fp)\n",
        "    for i in FS:\n",
        "        writer.writerow([i[0], i[1], i[2]])\n",
        "    fp.close()\n",
        "\n",
        "\n",
        "    predictions = np.argmin(FS,axis=1)\n",
        "    return predictions\n",
        "\n",
        "def getfile(filename):\n",
        "    root='/content/drive/My Drive/Colab Notebooks/csv/'\n",
        "    file = root+filename\n",
        "    if '.csv' not in filename:\n",
        "        file = file+'.csv'\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    df = np.asarray(df)\n",
        "\n",
        "    file = root+'labels.csv'\n",
        "    labels = pd.read_csv(file, header=None)\n",
        "    labels = np.asarray(labels)\n",
        "    return df,labels\n",
        "\n",
        "p1,labels = getfile(\"xception\")\n",
        "p2,_ = getfile(\"inceptionresnetv2\")\n",
        "p3,_ = getfile(\"mobilenetv2\")\n",
        "\n",
        "predictions = Mitcherlich(2, p1, p2, p3)\n",
        "for i in predictions:\n",
        "    print(i, end=\" \")\n",
        "print()\n",
        "\n",
        "correct = np.where(predictions == labels)[0].shape[0]\n",
        "total = labels.shape[0]\n",
        "\n",
        "classes = ['1', '2']\n",
        "\n",
        "metrics(labels, predictions, classes)\n",
        "\n",
        "plot_roc(labels,predictions)\n",
        "\n",
        "\n",
        "avg_acc_list = []\n",
        "avg_precision_list = []\n",
        "avg_recall_list = []\n",
        "avg_f1_list = []\n",
        "\n",
        "acc_fold = accuracy_score(labels, predictions)\n",
        "avg_acc_list.append(acc_fold)\n",
        "precision_fold = precision_score(labels, predictions, average='macro')\n",
        "avg_precision_list.append(precision_fold)\n",
        "recall_fold = recall_score(labels, predictions, average='macro')\n",
        "avg_recall_list.append(recall_fold)\n",
        "f1_fold  = f1_score(labels, predictions, average='macro')\n",
        "avg_f1_list.append(f1_fold)\n",
        "\n",
        "print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
        "print('________________________________________________________________')\n",
        "\n",
        "avg_acc = np.asarray(avg_acc_list)\n",
        "avg_precision = np.asarray(avg_precision_list)\n",
        "avg_recall = np.asarray(avg_recall_list)\n",
        "avg_f1 = np.asarray(avg_f1_list)\n",
        "print(\"\\n\")\n",
        "print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_precision), np.mean(avg_recall), np.mean(avg_f1)))"
      ],
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.status.busy": "2023-09-04T05:30:24.339068Z",
          "iopub.execute_input": "2023-09-04T05:30:24.339451Z",
          "iopub.status.idle": "2023-09-04T05:30:25.388576Z",
          "shell.execute_reply.started": "2023-09-04T05:30:24.339418Z",
          "shell.execute_reply": "2023-09-04T05:30:25.387579Z"
        },
        "trusted": true,
        "id": "sBpOwaCiwxuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KffHnnQycKEt",
        "IXKsyAjnY7sK",
        "jNjz3dlbZPEL",
        "tgRByXkyZ3Ku",
        "elLENXLqrgrb"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}